### Starting the Spark Master Server:

To start the Spark master, enter one of the following commands in the terminal:

```bash
start-master.sh
```

Or specify the IP address and port:

```bash
start-master.sh --host <IP ADDRESS> --port 7077
```

Then, open a web browser and navigate to:

```bash
http://<IP ADDRESS>:8080
```

This page will display the Spark URL, worker status, hardware resource usage, and other details.

---

### Starting a Worker Process:

To start a worker node, run the following command in the terminal:

```bash
start-worker.sh spark://<master-ip>:7077
```

---

### Starting a Worker Process with Specific Resource Allocation (Cores):

By default, Spark workers use all available CPU cores. To limit the number of cores, use the `-c` flag. For example, to assign only one core:

```bash
start-worker.sh -c 1 spark://<master-ip>:7077
```

Reload the Spark Master Web UI to confirm the worker's configuration.

---

### Starting a Worker with Specific Memory Allocation:

You can allocate a specific amount of memory to a worker using the `-m` flag. By default, Spark uses all available memory minus 1GB. For instance, to assign 512MB of memory:

```bash
start-worker.sh -m 512M spark://<master-ip>:7077
```

Or, to start a worker with 2 CPU cores and 1GB of memory:

```bash
start-worker.sh -c 2 -m 1G spark://<master-ip>:7077
```

Reload the Spark Master Web UI to check the worker's memory and core configuration.

---

### Writing and Running Spark Applications on the Cluster:

#### Step 1: Writing a PySpark Script (example.py):

```python
from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder \
    .appName("ExampleApp") \
    .config("spark.master", "spark://master-node-ip:7077") \
    .getOrCreate()

# Define your task or job
data = [('Alice', 1), ('Bob', 2), ('Cathy', 3)]
df = spark.createDataFrame(data, ['Name', 'Age'])
df.show()

# Stop the SparkSession
spark.stop()
```

#### Step 2: Running the PySpark Script:

To submit the PySpark script to the cluster:

```bash
spark-submit --master spark://master-node-ip:7077 --num-executors 2 --executor-cores 1 example.py
```

---

### Stopping Spark Processes:

- To stop the master node:

```bash
stop-master.sh
```

- To stop a worker node:

```bash
stop-worker.sh
```

---

### Starting and Stopping All Instances:

- To start both the master and all workers:

```bash
start-all.sh
```

- To stop all running instances:

```bash
stop-all.sh
```

---

### Starting Spark in the Terminal:

- For Scala:

```bash
spark-shell
```

- For Python:

```bash
pyspark
```

To view the Spark Web UI for individual jobs:

```bash
http://<IP ADDRESS>:4040
```

This interface will display the job status, worker details, and resource usage.
