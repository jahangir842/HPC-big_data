### Reference:
- https://medium.com/@agusmahari/pyspark-step-by-step-guide-to-installing-pyspark-on-linux-bb8af96ea5e8

---

# Apache Spark (Processing Engine)
## How to Use Apache Spark

### 1. Setup and Installation
To start using Apache Spark, you need to set up a Spark cluster or run it locally on your machine. Here are the steps for a local setup:

#### Install Java
Latest OpenJDK is not compatible with Spark 3.1.2. Spark 3.1.2 typically supports Java versions 8 or 11, and using newer versions like Java 21 can lead to the errors you're seeing. :

```sh
sudo apt update
sudo apt install openjdk-11-jdk -y
```

#### Download and Install Apache Spark
```sh
wget https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
tar -xvzf spark-3.1.2-bin-hadoop3.2.tgz
sudo mv spark-3.1.2-bin-hadoop3.2 /opt/spark
sudo chmod -R 777 /opt/spark
```

#### Update `bashrc`

Rename the java version accordingly.

```sh
echo "export SPARK_HOME=/opt/spark" >> ~/.bashrc
echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/' >> ~/.bashrc
echo "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin:$JAVA_HOME/bin" >> ~/.bashrc
source ~/.bashrc
```

---

### Options 1: Install a Virtual Environment (for Pip)
When you activate a Python virtual environment, tools like pip become available because the `virtual environment` includes its isolated installation of pip
**Create a virtual environment**:

   ```bash
   sudo apt install python3-venv
   python3 -m venv myenv
   ```

**Activate the virtual environment**:

   ```bash
   source myenv/bin/activate
   ```

### Options 2: Install Python 3 Pip (Systemwise Pip)
This is the recommended approach to avoid interfering with the system-wide Python installation.
Out of `virtual environment` if pip (Python package installer) is not already installed, you can install it using:

```sh
sudo apt install python3-pip
```

#### **Install `pyspark` inside the virtual environment**:
   ```bash
   pip install pyspark
   ```

#### **Use the environment**:
   Now, you can use `pyspark` while the virtual environment is activated. When you're done, deactivate the environment by typing `deactivate`.

#### Verify Installation
```sh
pyspark
```

### 3. Start Spark in Terminal
- Scala: `spark-shell`
- Python: `pyspark`

To view the Spark Web UI:  
`http://<IP_ADDRESS>:4040`

### 4. Start Standalone Spark Master Server
In the terminal, type:
```sh
start-master.sh
```
Access the web UI at `http://<IP_ADDRESS>:8080`

---

### Configure Spark
Edit the `conf/spark-env.sh` file on the master node. If it doesnâ€™t exist, create it from the template:
```sh
cp /opt/spark/conf/spark-env.sh.template /opt/spark/conf/spark-env.sh
nano /opt/spark/conf/spark-env.sh
```
Add the following configuration:
```sh
export SPARK_MASTER_HOST='master-node-ip'
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

Create a `workers` file in the `conf` directory on the master node and add the IP addresses of your worker nodes:
```sh
cp /opt/spark/conf/worker.template /opt/spark/conf/worker
echo 'worker-node-1-ip' >> /opt/spark/conf/workers
echo 'worker-node-2-ip' >> /opt/spark/conf/workers
echo 'worker-node-3-ip' >> /opt/spark/conf/workers
```


---

### Summary of Installations
- **Core Spark Installation**: The tar file provides the core Spark installation, including all necessary binaries and configuration files needed to run Spark.
- **Python Integration**: Installing PySpark with `pip3` sets up the Python bindings and dependencies, making it possible to use Spark with Python and ensuring compatibility with other Python packages.

This dual approach ensures that your Spark installation is complete and that you can effectively use Spark within your Python environment.

### Generate SSH Key Pair on Master Node and Send to All Workers
Passwordless SSH access is needed on the master node to allow seamless communication and task distribution between the master and worker nodes in a Spark cluster. It enables the master node to remotely execute commands and manage the worker nodes without manual password entry, facilitating automated cluster operations and job execution.

#### Generate SSH Key Pair
```sh
ssh-keygen -t rsa -b 4096 -C "your_email@example.com"
```

Ensure the SSH agent is running (optional but recommended for managing keys):
```sh
eval "$(ssh-agent -s)"
```

Add your private key to the SSH agent:
```sh
ssh-add ~/.ssh/id_rsa
```

#### Copy the Public Key to Worker Nodes
```sh
ssh-copy-id user@worker-node-1-ip
ssh-copy-id user@worker-node-2-ip
ssh-copy-id user@worker-node-3-ip
```
Replace `user` with your actual username on the worker nodes.

You should be able to log in to each worker node without being prompted for a password.


---
---
---



#### Step 4: Configure SSH Access
On the master node, generate an SSH key and distribute it to each worker node for passwordless access:

```bash
ssh-keygen -t rsa -P ""
ssh-copy-id -i ~/.ssh/id_rsa.pub user@worker-node-1
ssh-copy-id -i ~/.ssh/id_rsa.pub user@worker-node-2
ssh-copy-id -i ~/.ssh/id_rsa.pub user@worker-node-3
```

#### Step 5: Configure Spark
On the master node, modify the `conf/spark-env.sh` file to configure the Spark environment. If the file doesn't exist, create it from the template:

```bash
cp /opt/spark/conf/spark-env.sh.template /opt/spark/conf/spark-env.sh
```

Add the following configuration to set the master node IP and Java environment:

```bash
export SPARK_MASTER_HOST='master-node-ip'
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

Then, create a `slaves` file in the `conf` directory on the master node and add the IP addresses of your worker nodes:

```bash
echo 'worker-node-1-ip' >> /opt/spark/conf/slaves
echo 'worker-node-2-ip' >> /opt/spark/conf/slaves
echo 'worker-node-3-ip' >> /opt/spark/conf/slaves
```

#### Step 6: Start the Cluster
On the master node, start the Spark master process:

```bash
/opt/spark/sbin/start-master.sh
```

Next, on each worker node, start the worker process and connect it to the master node:

```bash
/opt/spark/sbin/start-slave.sh spark://master-node-ip:7077
```

#### Step 7: Verify the Setup
To check if the cluster is running, open a web browser and go to `http://master-node-ip:8080`. You should see the Spark master web interface showing the connected worker nodes.

#### Step 8: Submit a Spark Job
Create a simple Python script named `example.py` to test your setup:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("ExampleApp").getOrCreate()
data = [('Alice', 1), ('Bob', 2), ('Cathy', 3)]
df = spark.createDataFrame(data, ['Name', 'Age'])
df.show()
spark.stop()
```

Submit this job from the master node using the following command:

```bash
/opt/spark/bin/spark-submit --master spark://master-node-ip:7077 example.py
```

---
